[DEFAULT]
# The value in `DEFAULT` section will be referenced by other sections.
# For convinence, we will put the variables which changes frequently here and 
# let other section refer them
sample_seed: 1234
debug_mode: False
#reader: dataset |  pyreader | async | datafeed | sync
#data_reader: dataset
data_reader: pyreader
dataset_mode: Memory
py_reader_iterable: False

CUDA_VISIBLE_DEVICES: 0
#FLAGS_fraction_of_gpu_memory_to_use: 0.2

#for predict, init_pretrain_model prior to eval_dir, and can change the net by train saved
#init_pretrain_model: ../tmp/model/lr/save_model/checkpoint_final
#thread_num: 10
#print_period: 200

#local-cpu | local-gpu | pserver-local
platform: local-cpu
# Input settings
dataset_name: LinearRegression
#file_list prior to dataset_dir
file_list: ./test/linear_regression.data
dataset_dir: ../tmp/data/lr
reader_batch: False
drop_last_batch: False

# Model settings
model_name: LinearRegression
#file_pattern: %s-part-*
file_pattern: part-

#this network args
input_size: 13
vocab_size: 10000
emb_dim: 128
num_samples_train: 100
#train_batch_size: 100 
train_batch_size: 3
num_samples_eval: 10
eval_batch_size: 1

#for input dim
num_in_dimension: 3
#for output dim
num_out_dimension: 1


[Train]
#######################
#  Dataset Configure  #
#######################
# dataset_split_name
dataset_split_name: train

# Learning options
base_lr: 0.01
max_number_of_steps: None
#init_train_params: ../tmp/ernie/models/ernie_v2_chn_base/ernie_params
# Number of epochs from dataset source
num_epochs_input: 100

# The file type or record
file_type: record

# The number of input sample for training
num_samples: ${num_samples_train}

# The number of parallel readers that read data from the dataset
num_readers: 1

# The number of threads used to create the batches
num_preprocessing_threads: 1

batch_shuffle_size: 0

###########################
#  Basic Train Configure  #
###########################
# Directory where checkpoints and event logs are written to.
train_dir: ../tmp/model/lr/save_model
# The max number of ckpt files to store variables
save_max_to_keep: 40

# The frequency with which the model is saved, in steps.
save_model_steps: 100

#####################################
#  Training Optimization Configure  #
#####################################
# The number of samples in each batch
batch_size: ${train_batch_size}

# The weight decay on the model weights
#weight_decay: 0.00000001
weight_decay: None

# The amount of label smoothing
label_smoothing: 0.0

# The decay to use for the moving average. If left as None, then moving averages are not used
moving_average_decay: None

# ***************** learning rate options ***************** #
# Specifies how the learning rate is decayed. One of "fixed", "exponential" or "polynomial"
learning_rate_decay_type: fixed 

# Learning rate decay factor
learning_rate_decay_factor: 0.1

# Proportion of training steps to perform linear learning rate warmup for
learning_rate_warmup_proportion: 0.0

init_learning_rate: 0

learning_rate_warmup_steps: 0

# The minimal end learning rate used by a polynomial decay learning rate
end_learning_rate: 0.0

# Number of epochs after which learning rate decays
num_epochs_per_decay: 10

# A boolean, whether or not it should cycle beyond decay_steps
learning_rate_polynomial_decay_cycle: False

# ******************* optimizer options ******************* #
# The name of the optimizer, one of the following:
# "adadelta", "adagrad", "adam", "ftrl", "momentum", "sgd" or "rmsprop"
optimizer: adam
#optimizer: sgd
# Epsilon term for the optimizer, used for adadelta, adam, rmsprop
opt_epsilon: 1e-6

# conf for adadelta
# The decay rate for adadelta
adadelta_rho: 0.95
# Starting value for the AdaGrad accumulators
adagrad_initial_accumulator_value: 0.1

# conf for adam
# The exponential decay rate for the 1st moment estimates
adam_beta1: 0.9
# The exponential decay rate for the 2nd moment estimates
adam_beta2: 0.999

# conf for ftrl
# The learning rate power
ftrl_learning_rate_power: -0.1
# Starting value for the FTRL accumulators
ftrl_initial_accumulator_value: 0.1
# The FTRL l1 regularization strength
ftrl_l1: 0.0
# The FTRL l2 regularization strength
ftrl_l2: 0.01

# conf for momentum
# The momentum for the MomentumOptimizer and RMSPropOptimizer
momentum: 0.9

# conf for rmsprop
# Decay term for RMSProp
rmsprop_decay: 0.9

#############################
#  Log and Trace Configure  #
#############################
# The frequency with which logs are print
log_every_n_steps: 10


[Evaluate]
# process mode: pred, eval or export
#proc_name: eval
proc_name: pred

py_reader_iterable: True
platform: local-cpu

#######################
#  Dataset Configure  #
#######################
# The name of the train/test split
dataset_split_name: validation

# The directory where the dataset files are stored
dataset_dir: ../tmp/data/lr/test

# The file type or record
file_type: text

# The number of samples in each batch
batch_size: ${eval_batch_size}

# The number of input sample for evaluation
num_samples: ${num_samples_eval}

# The number of parallel readers that read data from the dataset
num_readers: 1

# The number of threads used to create the batches
num_preprocessing_threads: 1

# Number of epochs from dataset source
num_epochs_input: 1

# Directory where the results are saved to
eval_dir: ${Train:train_dir}/checkpoint_final

# Directory where the results are exported to
export_dir: ${Train:train_dir}

# The decay to use for the moving average. If left as None, then moving averages are not used.
moving_average_decay: None

